{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U ucimlrepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "ionosphere = fetch_ucirepo(id=52) \n",
    "# data (as pandas dataframes)\n",
    "X = ionosphere.data.features \n",
    "y = ionosphere.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = X.to_numpy()\n",
    "labels = np.array([0 if i=='g' else 1 for i in y.to_numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define GD ans SGD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(samples, labels, theta, lamb=0):\n",
    "    ''' Logistic regression loss '''\n",
    "    res = 0\n",
    "    # loss function \n",
    "    for x_n, y_n in zip(samples, labels):\n",
    "        res += np.log(1 + np.exp(-y_n * (x_n.T @ theta))) + (lamb/2) * np.linalg.norm(theta)**2\n",
    "\n",
    "    return res\n",
    "\n",
    "def grad_logistic(samples, labels, theta, lamb=0):\n",
    "    ''' Gradient of logistic regression loss '''\n",
    "    n_samp, n_feat = samples.shape\n",
    "    acc = 0\n",
    "    # for every sample\n",
    "    for x_n, y_n in zip(samples, labels):\n",
    "        e_term1 = 1/(1 + np.exp(y_n * (x_n.T @ theta)))\n",
    "        tmp = np.zeros(n_feat)\n",
    "        # compute partial derivatives\n",
    "        for idx, f in enumerate(x_n):\n",
    "            tmp[idx] = (e_term1 * -(y_n * f)) + lamb * theta[idx]\n",
    "        acc += tmp\n",
    "        \n",
    "    return acc/n_samp\n",
    "\n",
    "def sg_logistic(samples, labels, theta, rng, lamb=0, batch_size=1,):\n",
    "    ''' Stochastic gradient descent. Uses vanilla gradient descent with randomly chosen subset of samples. '''\n",
    "    n_samp, n_feat = samples.shape\n",
    "    # randomly choose samples according to batch_size\n",
    "    s = rng.integers(low=0, high=n_samp, size=batch_size)\n",
    "    sample = samples[s]\n",
    "    label = labels[s]\n",
    "    grad = grad_logistic(samples=sample, labels=label, theta=theta, lamb=lamb)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samp, n_feat = samples.shape\n",
    "rng = np.random.default_rng(42)\n",
    "theta = rng.uniform(high=1, low=-1, size=n_feat)\n",
    "lamb = 0.01\n",
    "lr = 0.5\n",
    "le = []\n",
    "for e in range(5000): # cant parallelize iter\n",
    "    loss_e = logistic_loss(samples=samples, labels=labels, theta=theta, lamb=lamb)\n",
    "    le.append(loss_e)\n",
    "    grad = sg_logistic(samples=samples, labels=labels, theta=theta, lamb=lamb, rng=rng, batch_size=50)\n",
    "    # grad = grad_logistic(samples=samples, labels=labels, theta=theta, lamb=lamb)\n",
    "    theta = theta - lr * grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with 'nan' and encode string labels\n",
    "std_scale = StandardScaler()\n",
    "samples = X.fillna('nan').apply(LabelEncoder().fit_transform).to_numpy()\n",
    "samples = std_scale.fit_transform(samples)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([0 if i=='<=50K' else 1 for i in y['income']]) # x <= 50K -> 0, x > 50K -> 1\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define worker GD and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_gd(my_id, samples, labels, lamb, seed, channel, batch_size): # channel isolated communication channel between worker and main process \n",
    "                                                # channel[1]=downlink channel[0]=uplink\n",
    "    ''' Worker function to compute GD and SGD '''\n",
    "    terminate = False\n",
    "    while True:\n",
    "        # block before getting data from server\n",
    "        while channel[1].empty(): # wait until received model parameters\n",
    "            continue\n",
    "        data = channel[1].get()\n",
    "        terminate = True if data is None else False\n",
    "        if terminate:\n",
    "            break\n",
    "        \n",
    "        theta = data\n",
    "        # compute gradient\n",
    "        n_samp, n_feat = samples.shape\n",
    "        # randomly select samples for SGD. Note is batch_size = total number of samples is do vanilla GD\n",
    "        if batch_size != n_samp:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            s = rng.integers(low=0, high=n_samp, size=batch_size)\n",
    "            samples = samples[s]\n",
    "            labels = labels[s]\n",
    "        # compute gradient\n",
    "        acc_grad = 0\n",
    "        for x_n, y_n in zip(samples, labels):\n",
    "            e_term1 = 1/(1 + np.exp(y_n * (x_n.T @ theta)))\n",
    "            tmp = np.zeros(n_feat)\n",
    "            for idx, f in enumerate(x_n):\n",
    "                tmp[idx] = (e_term1 * -(y_n * f)) + lamb * theta[idx]\n",
    "            acc_grad += tmp\n",
    "            \n",
    "        grad_m = acc_grad\n",
    "        upload = (my_id, grad_m)\n",
    "        # send results back to server\n",
    "        channel[0].put(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">WARNING</span>.: If training terminated by user, worker processes are still running in the background. In this case, manually kill process using command line terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialize variables\n",
    "n_samp, n_feat = samples.shape\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "theta = rng.uniform(high=1, low=-1, size=n_feat)\n",
    "lamb = 0.01\n",
    "lr = 0.01\n",
    "M = 4\n",
    "\n",
    "# create processes and distribute. process will be waiting until data appears in downlink\n",
    "split_samples = np.array_split(samples, M, axis=0)\n",
    "split_labels = np.array_split(labels, M, axis=0)\n",
    "processes = []\n",
    "comm_channels = []\n",
    "for i in range(M):\n",
    "    channel = [mp.Queue(), mp.Queue()]\n",
    "    batch_size = split_samples[i].shape[0] # uncomment for vanilla GD\n",
    "    # batch_size = 20 # uncomment for SGD\n",
    "    process = mp.Process(target=worker_gd, args=(i, split_samples[i], split_labels[i], lamb, seed, channel, batch_size))\n",
    "    processes.append(process)\n",
    "    comm_channels.append(channel)\n",
    "    process.start()\n",
    "\n",
    "# run\n",
    "for e in range(200): \n",
    "    loss_e = logistic_loss(samples=samples, labels=labels, theta=theta, lamb=lamb)\n",
    "    print(loss_e)\n",
    "    # send model to workers throught comm_channels\n",
    "    for id in range(M): # channel[0] = uplink  channel[1] = downlink\n",
    "        donwlink = comm_channels[id][1]\n",
    "        donwlink.put(theta)\n",
    "\n",
    "    # wait until all workers has sent their gradients\n",
    "    num_workers_ready = 0\n",
    "    w_id = 0\n",
    "    acc_grad = 0\n",
    "    while num_workers_ready < M: # check if all worker have sent gradient\n",
    "        uplink = comm_channels[w_id][0]\n",
    "        if not uplink.empty():\n",
    "            g = uplink.get() # -> (id, gradient)\n",
    "            acc_grad += g[1]\n",
    "            num_workers_ready = num_workers_ready + 1\n",
    "\n",
    "        w_id = (w_id + 1) % M\n",
    "    # gradient descent\n",
    "    grad = acc_grad/n_samp\n",
    "    theta = theta - lr * grad\n",
    "\n",
    "# terminate workers by sending None\n",
    "for id in range(M): # channel[0] = uplink  channel[1] = downlink\n",
    "        donwlink = comm_channels[id][1]\n",
    "        donwlink.put(None)\n",
    "        processes[id].join()\n",
    "        processes[id].terminate()\n",
    "        # print(f'Is {processes[id]} alive? {processes[id].is_alive()}') # check if process still alive      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
